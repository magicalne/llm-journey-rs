use candle_nn::Activation;

// https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct/blob/main/config.json
#[derive(Debug, Clone)]
pub struct RopeScaling {
    pub beta_fast: i32,
    pub beta_slow: i32,
    pub factor: i32,
    pub mscale: f32,
    pub mscale_all_dim: f32,
    pub original_max_position_embeddings: i32,
    pub typ: String,
}

#[derive(Debug, Clone)]
pub struct ModelConfig {
    pub architectures: Vec<String>,
    pub attention_bias: bool,
    pub attention_dropout: f32,
    pub aux_loss_alpha: f32,
    pub pad_token_id: Option<i32>,
    pub bos_token_id: i32,
    pub eos_token_id: i32,
    pub first_k_dense_replace: i32,
    pub hidden_act: Activation,
    pub hidden_size: usize,
    pub initializer_range: f32,
    pub intermediate_size: i32,
    pub kv_lora_rank: i32,
    pub max_position_embeddings: usize,
    pub model_type: String,
    pub moe_intermediate_size: usize,
    pub moe_layer_freq: i32,
    pub n_group: i32,
    pub n_routed_experts: i32,
    pub n_shared_experts: usize,
    pub norm_topk_prob: bool,
    pub num_attention_heads: usize,
    pub num_experts_per_tok: i32,
    pub num_hidden_layers: usize,
    pub num_key_value_heads: usize,
    pub pretraining_tp: i32,
    pub q_lora_rank: Option<i32>,
    pub qk_nope_head_dim: i32,
    pub qk_rope_head_dim: i32,
    pub rms_norm_eps: f64,
    pub rope_scaling: RopeScaling,
    pub rope_theta: i32,
    pub routed_scaling_factor: f32,
    pub scoring_func: String,
    pub seq_aux: bool,
    pub tie_word_embeddings: bool,
    pub topk_group: i32,
    pub topk_method: String,
    pub torch_dtype: String,
    pub transformers_version: String,
    pub use_cache: bool,
    pub v_head_dim: i32,
    pub vocab_size: i32,
    pub use_flash_attn: bool,

    // training configs
    pub ep_size: Option<usize>,
}

impl ModelConfig {
    pub fn deepseek_coder_v2_lite_instruct() -> Self {
        Self {
            architectures: vec!["DeepseekV2ForCausalLM".to_string()],
            attention_bias: false,
            attention_dropout: 0.0,
            aux_loss_alpha: 0.001,
            pad_token_id: None,
            bos_token_id: 100000,
            eos_token_id: 100001,
            first_k_dense_replace: 1,
            hidden_act: Activation::Silu,
            hidden_size: 2048,
            initializer_range: 0.02,
            intermediate_size: 10944,
            kv_lora_rank: 512,
            max_position_embeddings: 163840,
            model_type: "deepseek_v2".to_string(),
            moe_intermediate_size: 1408,
            moe_layer_freq: 1,
            n_group: 1,
            n_routed_experts: 64,
            n_shared_experts: 2,
            norm_topk_prob: false,
            num_attention_heads: 16,
            num_experts_per_tok: 6,
            num_hidden_layers: 27,
            num_key_value_heads: 16,
            pretraining_tp: 1,
            q_lora_rank: None,
            qk_nope_head_dim: 128,
            qk_rope_head_dim: 64,
            rms_norm_eps: 1e-06,
            rope_scaling: RopeScaling {
                beta_fast: 32,
                beta_slow: 1,
                factor: 40,
                mscale: 0.707,
                mscale_all_dim: 0.707,
                original_max_position_embeddings: 4096,
                typ: "yarn".to_string(),
            },
            rope_theta: 10000,
            routed_scaling_factor: 1.0,
            scoring_func: "softmax".to_string(),
            seq_aux: true,
            tie_word_embeddings: false,
            topk_group: 1,
            topk_method: "greedy".to_string(),
            torch_dtype: "bfloat16".to_string(),
            transformers_version: "4.39.3".to_string(),
            use_cache: true,
            v_head_dim: 128,
            vocab_size: 102400,
            use_flash_attn: true,
            ep_size: None,
        }
    }
}
